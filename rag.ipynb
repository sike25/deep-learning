{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1xCp6n57BB2"
   },
   "source": [
    "# RAG + LLM Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmI3jBxK68-G"
   },
   "source": [
    "Your task is to create a Retrieval-Augmented Generation (RAG) system using a Large Language Model (LLM). The RAG system should be able to retrieve relevant information from a knowledge base and generate coherent and informative responses to user queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3STgq_s6_mV"
   },
   "source": [
    "Steps:\n",
    "\n",
    "1. Choose a domain and collect a suitable dataset of documents (at least 5 documents - PDFs or HTML pages) to serve as the knowledge base for your RAG system. Select one of the following topics:\n",
    "   * latest scientific papers from arxiv.org,\n",
    "   * fiction books released,\n",
    "   * legal documents or,\n",
    "   * social media posts.\n",
    "\n",
    "   Make sure that the documents are newer then the training dataset of the applied LLM. (20 points)\n",
    "\n",
    "2. Create three relevant prompts to the dataset, and one irrelevant prompt. (20 points)\n",
    "\n",
    "3. Load an LLM with at least 5B parameters. (10 points)\n",
    "\n",
    "4. Test the LLM with your prompts. The goal should be that without the collected dataset your model is unable to answer the question. If it gives you a good answer, select another question to answer and maybe a different dataset. (10 points)\n",
    "\n",
    "5. Create a LangChain-based RAG system by setting up a vector database from the documents. (20 points)\n",
    "\n",
    "6. Provide your three relevant and one irrelevant prompts to your RAG system. For the relevant prompts, your RAG system should return relevant answers, and for the irrelevant prompt, an empty answer. (20 points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9nwufKe08D3e"
   },
   "outputs": [],
   "source": [
    "hugging_face_token = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
